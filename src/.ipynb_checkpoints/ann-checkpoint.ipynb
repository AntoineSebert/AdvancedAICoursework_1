{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 784\n",
    "hidden_nodes = 400\n",
    "output_nodes = 10\n",
    "\n",
    "# initial values\n",
    "learning_rate = 0.2\n",
    "batch_size = 40\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "\t\"\"\"Artificial Neural Network classifier.\n",
    "\n",
    "\tParameters\n",
    "\t------------\n",
    "\tlr : float\n",
    "\t\tLearning rate (between 0.0 and 1.0)\n",
    "\tep : int\n",
    "\t\tNumber of epochs for training the network towards achieving convergence\n",
    "\tbatch_size : int\n",
    "\t\tSize of the training batch to be used when calculating the gradient descent.\n",
    "\t\tbatch_size = 0 standard gradient descent\n",
    "\t\tbatch_size > 0 stochastic gradient descent\n",
    "\n",
    "\tinodes : int\n",
    "\t\tNumber of input nodes which is normally the number of features in an instance.\n",
    "\thnodes : int\n",
    "\t\tNumber of hidden nodes in the net.\n",
    "\tonodes : int\n",
    "\t\tNumber of output nodes in the net.\n",
    "\n",
    "\tAttributes\n",
    "\t-----------\n",
    "\twih : 2d-array\n",
    "\t\tInput2Hidden node weights after fitting\n",
    "\twho : 2d-array\n",
    "\t\tHidden2Output node weights after fitting\n",
    "\tE : list\n",
    "\t\tSum-of-squares error value in each epoch.\n",
    "\n",
    "\tResults : list\n",
    "\t\tTarget and predicted class labels for the test data.\n",
    "\n",
    "\tFunctions\n",
    "\t---------\n",
    "\tactivation_function : float (between 1 and -1)\n",
    "\t\timplments the sigmoid function which squashes the node input\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, inputnodes = 784, hiddennodes = 200, outputnodes = 10, learningrate = 0.2, batch_size = 40, epochs = 100):\n",
    "\t\tself.inodes = inputnodes\n",
    "\t\tself.hnodes = hiddennodes\n",
    "\t\tself.onodes = outputnodes\n",
    "\n",
    "\t\t#link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "\t\t#a weight on link from node i to node j is w_ij\n",
    "\n",
    "\t\t#Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "\t\t#numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want)\n",
    "\t\t#scale is usually set to the standard deviation which is related to the number of incoming links i.e.\n",
    "\t\t#1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "\t\t#We have set 0 as the centre of the guassian dist.\n",
    "\t\t# size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "\t\tself.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "\t\tself.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "\t\t#set the learning rate\n",
    "\t\tself.lr = learningrate\n",
    "\n",
    "\t\t#set the batch size\n",
    "\t\tself.bs = batch_size\n",
    "\n",
    "\t\t#set the number of epochs\n",
    "\t\tself.ep = epochs\n",
    "\n",
    "\t\t#store errors at each epoch\n",
    "\t\tself.E = []\n",
    "\n",
    "\t\t#store results from testing the model\n",
    "\t\t#keep track of the network performance on each test instance\n",
    "\t\tself.results = []\n",
    "\n",
    "\t\t#define the activation function here\n",
    "\t\t#specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "\t\t#lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "\t\tself.activation_function = lambda x: scipy.special.expit(x)\n",
    "\n",
    "\t\tpass\n",
    "\n",
    "\tdef batch_input(self, input_list):\n",
    "\t\t\"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "\t\tfor i in range(0, len(input_list), self.bs):\n",
    "\t\t\tyield input_list[i:i + self.bs]\n",
    "\n",
    "\t#train the neural net\n",
    "\tdef train(self, train_inputs):\n",
    "\t\t\"\"\"Training the neural net.\n",
    "\t\t\tThis includes the forward pass ; error computation;\n",
    "\t\t\tbackprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "\t\t\tParameters\n",
    "\t\t\t----------\n",
    "\t\t\ttrain_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "\t\t\tTraining vectors, where n_instances is the number of training instances and\n",
    "\t\t\tn_features is the number of features.\n",
    "\t\t\tNote this contains all features including the class feature which is in first position\n",
    "\n",
    "\t\t\tReturns\n",
    "\t\t\t-------\n",
    "\t\t\tself : object\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tfor e in range(self.ep):\n",
    "\t\t\tprint(\"Training epoch#: \", e)\n",
    "\t\t\tsum_error = 0.0\n",
    "\t\t\tfor batch in self.batch_input(train_inputs):\n",
    "\t\t\t\t#creating variables to store the gradients\n",
    "\t\t\t\tdelta_who = 0\n",
    "\t\t\t\tdelta_wih = 0\n",
    "\n",
    "\t\t\t\tfor instance in batch:\n",
    "\n",
    "\t\t\t\t\t# split it by the commas\n",
    "\t\t\t\t\tall_values = instance.split(',')\n",
    "\t\t\t\t\t# scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "\t\t\t\t\t# divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "\t\t\t\t\t# multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "\t\t\t\t\t# add 0.01 to shift them up to the desired range 0.01 - 1.\n",
    "\t\t\t\t\tinputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "\t\t\t\t\t#create the target output values for each instance so that we can use it with the neural net\n",
    "\t\t\t\t\t#note we need 10 nodes where each represents one of the digits\n",
    "\t\t\t\t\ttargets = numpy.zeros(output_nodes) + 0.01 #all initialised to 0.01\n",
    "\t\t\t\t\t#all_value[0] has the target class label for this instance\n",
    "\t\t\t\t\ttargets[int(instance[0])] = 0.99\n",
    "\n",
    "\t\t\t\t\t#convert  inputs list to 2d array\n",
    "\t\t\t\t\tinputs = numpy.array(inputs,  ndmin = 2).T\n",
    "\t\t\t\t\ttargets = numpy.array(targets, ndmin = 2).T\n",
    "\n",
    "\t\t\t\t\t#calculate signals into hidden layer\n",
    "\t\t\t\t\thidden_inputs = numpy.dot(self.wih, inputs)\n",
    "\t\t\t\t\t#calculate the signals emerging from the hidden layer\n",
    "\t\t\t\t\thidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "\t\t\t\t\t#calculate signals into final output layer\n",
    "\t\t\t\t\tfinal_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "\t\t\t\t\t#calculate the signals emerging from final output layer\n",
    "\t\t\t\t\tfinal_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "\t\t\t\t\t#to calculate the error we need to compute the element wise diff between target and actual\n",
    "\t\t\t\t\toutput_errors = targets - final_outputs\n",
    "\t\t\t\t\t#Next distribute the error to the hidden layer such that hidden layer error\n",
    "\t\t\t\t\t#is the output_errors, split by weights, recombined at hidden nodes\n",
    "\t\t\t\t\thidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "\n",
    "\t\t\t\t\t## for each instance accumilate the gradients from each instance\n",
    "\t\t\t\t\t## delta_who are the gradients between hidden and output weights\n",
    "\t\t\t\t\t## delta_wih are the gradients between input and hidden weights\n",
    "\t\t\t\t\tdelta_who += numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "\t\t\t\t\tdelta_wih += numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "\n",
    "\t\t\t\t\tsum_error += numpy.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\t\t# update the weights by multiplying the gradient with the learning rate\n",
    "\t\t\t\t# note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "\t\t\t\t# obviously if batch size = 1 then we dont need to bother with an average\n",
    "\t\t\t\tself.who += self.lr * (delta_who / self.bs)\n",
    "\t\t\t\tself.wih += self.lr * (delta_wih / self.bs)\n",
    "\t\t\tpass # batch\n",
    "\t\t\tself.E.append(numpy.asfarray(sum_error).flatten())\n",
    "\t\t\tprint(\"errors (SSE): \", self.E[-1])\n",
    "\t\tpass # epoch\n",
    "\n",
    "\t#query the neural net\n",
    "\tdef query(self, inputs_list):\n",
    "\t\t#convert inputs_list to a 2d array\n",
    "\t\t#print(numpy.matrix(inputs_list))\n",
    "\t\t#inputs_list [[ 1.   0.5 -1.5]]\n",
    "\t\tinputs = numpy.array(inputs_list, ndmin = 2).T\n",
    "\t\t#once converted it appears as follows\n",
    "\t\t#[[ 1. ]\n",
    "\t\t# [ 0.5]\n",
    "\t\t# [-1.5]]\n",
    "\t\t#print(numpy.matrix(inputs))\n",
    "\n",
    "\t\t#propogate input into hidden layer. This is the start of the forward pass\n",
    "\t\thidden_inputs = numpy.dot(self.wih, inputs)\n",
    "\n",
    "\t\t#squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "\t\thidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "\t\t#propagate into output layer and the apply the squashing sigmoid function\n",
    "\t\tfinal_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "\n",
    "\t\tfinal_outputs = self.activation_function(final_inputs)\n",
    "\t\treturn final_outputs\n",
    "\n",
    "\t#iterate through all the test data to calculate model accuracy\n",
    "\tdef test(self, test_inputs):\n",
    "\t\tself.results = []\n",
    "\n",
    "\t\t#go through each test instances\n",
    "\t\tfor instance in test_inputs:\n",
    "\t\t\tall_values = instance.split(',') # extract the input feature values for the instance\n",
    "\n",
    "\t\t\ttarget_label = int(all_values[0]) # get the target class for the instance\n",
    "\n",
    "\t\t\t#scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "\t\t\tinputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "\t\t\t#query the network with test inputs\n",
    "\t\t\t#note this returns 10 output values ; of which the index of the highest value\n",
    "\t\t\t# is the networks predicted class label\n",
    "\t\t\toutputs = self.query(inputs)\n",
    "\n",
    "\t\t\t#get the index of the highest output node as this corresponds to the predicted class\n",
    "\t\t\tpredict_label = numpy.argmax(outputs) #this is the class predicted by the ANN\n",
    "\n",
    "\t\t\tself.results.append([predict_label, target_label])\n",
    "\t\t\t#compute network error\n",
    "\t\t\t#if (predict_label == target_label):\n",
    "\t\t\t#\tself.results.append(1)\n",
    "\t\t\t#else:\n",
    "\t\t\t#\tself.results.append(0)\n",
    "\t\t\tpass\n",
    "\t\tpass\n",
    "\t\tself.results = numpy.asfarray(self.results) # flatten results to avoid nested arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "\n",
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  60000\n",
      "test set size:  10000\n"
     ]
    }
   ],
   "source": [
    "mnist_train_file = open(\"../datasets/mnist_train.csv\", 'r')\n",
    "mnist_train_list = mnist_train_file.readlines() \n",
    "mnist_train_file.close() \n",
    "print(\"train set size: \", len(mnist_train_list))\n",
    "\n",
    "mnist_test_file = open(\"../datasets/mnist_test.csv\", 'r')\n",
    "mnist_test_list = mnist_test_file.readlines()\n",
    "mnist_test_file.close()\n",
    "print(\"test set size: \", len(mnist_test_list))\n",
    "\n",
    "# quick run\n",
    "mnist_train_list = numpy.random.choice(mnist_train_list, 1000, replace = False)\n",
    "mnist_test_list = numpy.random.choice(mnist_test_list, 1000, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor entry in mnist_train_list:\\n\\tall_values = entry.split(',')\\n\\tnormalized_data.append((numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01)\\nmnist_train_list = normalized_data\\nprint(normalized_data[0])\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data = []\n",
    "\"\"\"\n",
    "for entry in mnist_train_list:\n",
    "\tall_values = entry.split(',')\n",
    "\tnormalized_data.append((numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01)\n",
    "mnist_train_list = normalized_data\n",
    "print(normalized_data[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs :  1\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [638.84431313]\n",
      "Number of epochs :  10\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [742.70999774]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [317.66976716]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [253.9697914]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [221.1016366]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [199.86066287]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [184.62796603]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [172.96904941]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [163.61723705]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [155.84426622]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [149.20234353]\n",
      "Number of epochs :  100\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [713.32933604]\n",
      "Training epoch#:  1\n"
     ]
    }
   ],
   "source": [
    "ann_epoch_numbers_list = []\n",
    "epoch_numbers = [1, 10, 100, 200, 300]\n",
    "\n",
    "for epoch_number in epoch_numbers:\n",
    "    print(\"Number of epochs : \", epoch_number)\n",
    "    n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epoch_number)\n",
    "    n.train(mnist_train_list)\n",
    "    ann_epoch_numbers_list.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_numbers_model_results = []\n",
    "for model in ann_epoch_numbers_list: \n",
    "    correct = 0\n",
    "    model.test(mnist_test_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct / len(model.results))\n",
    "    epoch_numbers_model_results.append(correct)\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = epoch_numbers\n",
    "y_pos = numpy.arange(len(objects))\n",
    "performance = epoch_numbers_model_results\n",
    "\n",
    "plt.bar(y_pos, performance, align = 'center', alpha = 0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('epochs')\n",
    "\n",
    "fig_epoch_numbers = plt.gcf()\n",
    "plt.show()\n",
    "fig_epoch_numbers.savefig('../images/ann_epoch_numbers_mnist.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_batch_sizes_list = []\n",
    "batch_sizes = [1, 10, 100, 200, len(mnist_train_list)]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(\"Batch_sizes : \", batch_size)\n",
    "    n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epochs)\n",
    "    n.train(mnist_train_list)\n",
    "    ann_batch_sizes_list.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes_model_results = []\n",
    "for model in ann_batch_sizes_list: \n",
    "    correct = 0\n",
    "    model.test(mnist_test_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct/len(model.results))\n",
    "    batch_sizes_model_results.append(correct)\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = batch_sizes\n",
    "y_pos = numpy.arange(len(objects))\n",
    "performance = batch_sizes_model_results\n",
    "\n",
    "plt.bar(y_pos, performance, align = 'center', alpha = 0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('batch_size')\n",
    "\n",
    "fig_batch_sizes = plt.gcf()\n",
    "plt.show()\n",
    "fig_batch_sizes.savefig('../images/ann_batch_sizes_mnist.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_learning_rates_list = []\n",
    "learning_rates = [0.01, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"Learning_rates : \", learning_rate)\n",
    "    n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epochs)\n",
    "    n.train(mnist_train_list)\n",
    "    ann_learning_rates_list.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates_model_results = []\n",
    "for model in ann_learning_rates_list: \n",
    "    correct = 0\n",
    "    model.test(mnist_test_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct/len(model.results))\n",
    "    learning_rates_model_results.append(correct)\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning_rates\n",
    "y_pos = numpy.arange(len(objects))\n",
    "performance = learning_rates_model_results\n",
    "\n",
    "plt.bar(y_pos, performance, align = 'center', alpha = 0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('learning_rate')\n",
    "\n",
    "fig_learning_rates = plt.gcf()\n",
    "plt.show()\n",
    "fig_learning_rates.savefig('../images/ann_learning_rates_mnist.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Red and White Wine Quality EDA\" dataset\n",
    "\n",
    "### Loading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set size:  4899\n"
     ]
    }
   ],
   "source": [
    "wine_quality_file = open(\"../datasets/winequality-white.csv\", 'r')\n",
    "wine_quality_list = wine_quality_file.readlines()\n",
    "wine_quality_file.close()\n",
    "\n",
    "print(\"set size: \", len(wine_quality_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6\n",
      "\n",
      "7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine_quality_list[1])\n",
    "\"\"\"\n",
    "for entry in wine_quality_list:\n",
    "    entry = ?\n",
    "\"\"\"\n",
    "print(wine_quality_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into *training set* and *testing set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(wine_quality_list)\n",
    "\n",
    "wine_quality_train_list = wine_quality_list[:int(len(wine_quality_list) * .8)]\n",
    "print(\"train set size: \", len(wine_quality_train_list))\n",
    "\n",
    "wine_quality_test_list = wine_quality_list[int(len(wine_quality_list) * .8):] # why does it work ? should be 0.2 ! is there something I missed ? it's been 5 years I haven't used Python, mais quand même faut pas abuser...\n",
    "print(\"test set size: \", len(wine_quality_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate\n",
    "\n",
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the accuracy of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
